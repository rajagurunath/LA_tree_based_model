{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of Gradient Boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM\n",
    "# Catboost\n",
    "# XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of finding the best split \n",
    "\n",
    "## Think about Big O notations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Presort Algorithms\n",
    "## O(n(Features)*n(No of data points))\n",
    "## Traditional Implementations found in Sklearn  ,also used in Xgboost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to reduce this Time complexity in this algorithm when we are using Big datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![diff](https://i0.wp.com/mlexplained.com/wp-content/uploads/2018/01/DecisionTrees_3_thumb.png?w=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram Based Algorithms \n",
    "\n",
    "## IDEA 1\n",
    "\n",
    "## Use the Histograms to find the Bins for each feature \n",
    "## Use this bins to find the best split (This is supported by the fact that the Spliting on real value or bins does't cost much difference in accuracy)\n",
    "\n",
    "### Note : USing bins may also leads to prevent from Overfiting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![histogram](https://i2.wp.com/mlexplained.com/wp-content/uploads/2018/01/binned_split_gbdt.png?resize=300%2C260)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Based Strategy\n",
    "## IDEA 2\n",
    "\n",
    "## Use Gradients to find the best split !!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But How ??\n",
    "\n",
    "### What does the Large gradients and Small gradients with respect ro the Loss function tries to tell you?\n",
    "\n",
    "## Gradient-based One-Side Sampling (GOSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking Sparsity of the data as advantage\n",
    "\n",
    "## Ignoring sparse inputs (xgboost and lightGBM)\n",
    "### Xgboost proposes to ignore the 0 features when computing the split, then allocating all the data with missing values to whichever side of the split reduces the loss more. This reduces the number of samples that have to be used when evaluating each split, speeding up the training process.\n",
    "![](https://i2.wp.com/mlexplained.com/wp-content/uploads/2018/01/sparse_split_gbdt.png?resize=300%2C268)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exclusive Feature Bundling (lightGBM)\n",
    "### * some features are never non-zero together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference :\n",
    "- https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/\n",
    "- http://mlexplained.com/2018/01/05/lightgbm-and-xgboost-explained/\n",
    "- https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst\n",
    "- https://towardsdatascience.com/catboost-vs-light-gbm-vs-xgboost-5f93620723db"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
